{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "layers = tf.keras.layers\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n",
    "\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS = argparse.Namespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.render = True\n",
    "FLAGS.num_episodes = 500\n",
    "FLAGS.agent_type = 'REINFORCE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(tf.keras.Model):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(RandomAgent, self).__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        return\n",
    "    def act(self, state):\n",
    "        action = self.action_space.sample()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent(tf.keras.Model):\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        super(REINFORCEAgent, self).__init__()\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "        self.hidden_layer1 = layers.Dense(10,\n",
    "                                          activation='relu',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.hidden_layer2 = layers.Dense(self.action_space.n, \n",
    "                                          activation='relu',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.logit_layer = layers.Dense(self.action_space.n, activation='linear',\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    def act(self, state):\n",
    "        out = self.call(state)\n",
    "        sample_action = tf.random.multinomial(out, 1)\n",
    "        return sample_action[0, 0].numpy()\n",
    "    def call(self, state):\n",
    "        a = self.hidden_layer1(state)\n",
    "        a = self.hidden_layer2(a)\n",
    "        a = self.logit_layer(a)\n",
    "        return a\n",
    "        \n",
    "    def train_one_episode(self, trajectory):\n",
    "        episode_states  = [s for _, s, _, _, _ in trajectory]\n",
    "        episode_action_indexes = [a for _, _, a, _ , _ in trajectory]\n",
    "        episode_actions =[]\n",
    "        for a in episode_action_indexes:\n",
    "            one_hot = np.zeros(self.action_space.n)\n",
    "            one_hot[a] = 1\n",
    "            episode_actions.append(one_hot)\n",
    "        episode_rewards = [r for _, _, _, _, r in trajectory]\n",
    "        discounted_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "        episode_states = np.vstack(episode_states)\n",
    "        episode_actions = np.vstack(episode_actions)\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_logits = self(episode_states)\n",
    "            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=action_logits,labels=episode_actions)\n",
    "            loss = tf.reduce_mean(neg_log_prob * discounted_rewards)\n",
    "        grads = tape.gradient(loss, self.variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.GradientTape.gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = tf.constant([1, 2])\n",
    "v2 = tf.constant([3, 4])\n",
    "tf.reduce_sum([v1, v2], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.softmax(tf.constant([0.3, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.multinomial(tf.expand_dims(tf.log(probs), 0), 1)[0, 0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_env():\n",
    "    \"\"\"Returns configured gym env\"\"\"\n",
    "    #env = gym.make('CartPole-v0')\n",
    "    env = gym.make('Pendulum-v0')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(observation_space, action_space, agent_type):\n",
    "    \"\"\"Returns initialized agent\"\"\"\n",
    "    if agent_type == 'Random':\n",
    "        agent = RandomAgent(observation_space, action_space)\n",
    "    elif agent_type == 'REINFORCE':\n",
    "        agent = REINFORCEAgent(observation_space, action_space)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    envir = build_env()\n",
    "    agent = build_agent(envir.observation_space, \n",
    "                        envir.action_space, FLAGS.agent_type)\n",
    "    total_rewards = []\n",
    "    running_avg_reward = 0.0\n",
    "    for i in range(FLAGS.num_episodes):\n",
    "        done = False\n",
    "        obs = envir.reset()\n",
    "        steps = 0\n",
    "        total_reward = 0.0\n",
    "        trajectory = []\n",
    "        if FLAGS.render and i+1 == FLAGS.num_episodes:\n",
    "            frames = []\n",
    "        while not done:\n",
    "            action = agent.act(tf.expand_dims(obs, 0))\n",
    "            obs_prev = obs\n",
    "            obs, reward, done, info = envir.step(action)\n",
    "            traj_step = (steps, obs_prev, action, obs, reward)\n",
    "            trajectory.append(traj_step)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            if FLAGS.render and i+1 == FLAGS.num_episodes:\n",
    "                frames.append(envir.render(mode = 'rgb_array'))\n",
    "        # Render to GIF\n",
    "        if FLAGS.render and i+1 == FLAGS.num_episodes:\n",
    "            envir.render()\n",
    "            display_frames_as_gif(frames)\n",
    "        #train\n",
    "        running_avg_reward =  (total_reward + (i * running_avg_reward))/(i+1)\n",
    "        if i % 10 == 0:\n",
    "            print(running_avg_reward)\n",
    "        agent.train_one_episode(trajectory)\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = build_env()\n",
    "v = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = REINFORCEAgent(env.observation_space, env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gray/.conda/envs/deep-learning/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-42b61d0aef7c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0menvir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     agent = build_agent(envir.observation_space, \n\u001b[0;32m----> 4\u001b[0;31m                         envir.action_space, FLAGS.agent_type)\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtotal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mrunning_avg_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-141-fc8bb12f7c58>\u001b[0m in \u001b[0;36mbuild_agent\u001b[0;34m(observation_space, action_space, agent_type)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0magent_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'REINFORCE'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mREINFORCEAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-139-621b80091aa0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, observation_space, action_space)\u001b[0m\n\u001b[1;32m      7\u001b[0m                                           \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0;32m----> 9\u001b[0;31m         self.hidden_layer2 = layers.Dense(self.action_space.n, \n\u001b[0m\u001b[1;32m     10\u001b[0m                                           \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                           kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(cur_step, trajectory, gamma):\n",
    "    r = [(t, r) for t, _, _, _, r in trajectory]\n",
    "    return [(t, r * gamma**(i-cur_step)) for i, (t, r) in enumerate(r) if i>=cur_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards, gamma = 0.95):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    #mean = 0\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    #std = 1\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots = np.zeros((4, 3))\n",
    "idx = [1, 2, 0, 1]\n",
    "#one_hots[tuple(list(enumerate(idx)))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 2), (2, 0), (3, 1)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(enumerate(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-7bace80de542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_hots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "one_hots[tuple([(0, 1), (1, 2), (0, 2)])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4142135623730951"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(5).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hots[list(range(4)), [0, 1, 0, 2]] = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
